{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Seagull Story\n",
    "In this project we aim to fine-tune a BERT-like transformer to solve a Natural Language Inference task. The model is asked `yes`/`no` questions and has to answer `yes`, `no`, or `irrelevant` based on a given story.\n",
    "\n",
    "## The NLI task\n",
    "*Natural Language Inference*, also known as *textual entailment*, is the task of determining whether a *hypothesis* is true (**entailment**), false (**contradiction**), or undetermined (**neutral**) given a *premise*.\n",
    "\n",
    "In more precise terms, the *premise* $p$ entails the *hypothesis* $h$ ($p \\implies h$) if and only if, typically, a human reading $p$ would be justified in inferring the proposition expressed by $h$ from the proposition expressed by $p$. This is a more relaxed definition than the pure *logical entailment*. The **relation is directional** because even if $p \\implies h$, the reverse $h \\implies p$ is much less certain.\n",
    "\n",
    "Determining whether this relationship holds is an informal task, one which sometimes overlaps with the formal tasks of formal semantics. In this context, however, we want to tackle this task **using a BERT-like transformer**, which is now state-of-the-art for all kinds of NLP tasks.\n",
    "\n",
    "Read more on [Textual entailment][1].\n",
    "\n",
    "[1]:https://en.wikipedia.org/wiki/Textual_entailment\n",
    "\n",
    "### The choice of BERT\n",
    "When it comes to solving natural language tasks, the BERT and GPT models have come to the fore due to their innovative architectures. Both are based on the transformer architecture, which is based on the attention mechanism, discovered by Google in 2017 and published in the paper [Attention is All You Need](https://arxiv.org/abs/1706.03762) by Vaswani et al.\n",
    "\n",
    "Unlike GPT, BERT is trained to predict missing words in text, using both left and right contexts. This implies that BERT is a bidirectional moddel, and thus is superior for tasks that require understanding the context and nuances of language. In other words, BERT is better suited to tackle our task.\n",
    "\n",
    "Read more on [Differences Between GPT and BERT][2].\n",
    "\n",
    "[2]:https://www.geeksforgeeks.org/differences-between-gpt-and-bert/\n",
    "\n",
    "### The choice of a BERT-like model\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
